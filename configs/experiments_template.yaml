#---------------------------------------------------------------------------------
# - checkpoints_dir         : The directory path to save checkpoints.
# - tracking_dir            : The directory where the results of the experiments
# - experiments             : List of training experiments.
#                             will be stored.
# - data                    : Train and test data paths.
# - train_dir               : The directory path to the train data.
# - test_dir                : The directory path to the test data.
# - hyperparameters         : Training hyperparameters.
# - num_epochs              : An integer indicating how many epochs to train for.
# - batch_size              : How many samples per batch to load.
# - optimizer               : Hyperparameters for the optimizer.
# - learning_rate           : The learning rate of the optimizer.
# - optimizer_name          : The name of the optimizer to use for training.
#                             Available optimizers: `sgd`, `adam`.
# - weight_decay            : L2 penalty for the optimizers.
# - betas                   : Coefficients used for computing running averages
#                             of gradient and its square.
# - early_stopping          : Hyperparameters for early stopping.
# - patience                : Number of epochs to wait before early stopping.
# - delta                   : Minimum change in monitored quantity to qualify
#                             as an improvement.
# - model_name              : The name of the model to train. Available models:
#                             `tiny_vgg`, `efficient_net_b0`, `efficient_net_b2`,
#                             `vit`.
#---------------------------------------------------------------------------------
checkpoints_dir: ./checkpoints
tracking_dir: ./experiments/
experiments:
  -
    name: experiment_1
    data:
        train_dir: path_to_train_dir
        test_dir: path_to_test_dir
    hyperparameters:
      general:
        num_epochs: 1
        batch_size: 1
      optimizer:
        optimizer_name: sgd
        learning_rate: 0.001
        weight_decay: 0
        betas: (0.9, 0.999)
      early_stopping:
        patience: 5
        delta: 0
      model:
        model_name: tiny_vgg
  -
    name: experiment_2
    data:
        train_dir: path_to_train_dir
        test_dir: path_to_test_dir
    hyperparameters:
      general:
        num_epochs: 1
        batch_size: 1
      optimizer:
        optimizer_name: adam
        learning_rate: 0.001
        weight_decay: 0.3
        betas: (0.9, 0.999)
      early_stopping:
        patience: 5
        delta: 0
      model:
        model_name: efficient_net_b0
